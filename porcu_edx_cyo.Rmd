---
title: "HarvardX : Choose Your Own!"
subtitle: "Pima Indian Women Diabetes Classification"
author: "Jordan Porcu"
output: 
  pdf_document: 
    toc: yes
---

\newpage

# Introduction

## Overview

This document is done within the framework of the *HarvardX PH125.9x Data Science : Capstone* certification. This project is a chosen subject : Diabetes Binary Classification.

The data set we are going to use can be found at https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset.

In this project, as the subject implies, we will build a Machine learning model to predict if a woman has diabetes, based on physiological variables.

Since we face a binary classification problem, we will use two metrics to evaluate the models performance : **accuracy** & **f1-score**.

## The Dataset

The data set is given by the National Institute of Diabetes and Digestive and Kidney Diseases. It displays physiological attributes of 21 years old and above women from Pima Indian community. 

Variables are : 

-  **Pregnancies** : times a woman has been pregnant.
-  **Glucose** : glucose level in blood.
-  **BloodPressure** : blood pressure measurement.
-  **SkinThickness** : thickness of their skin.
-  **Insulin** : insulin level in blood.
-  **BMI** : body mass index.
-  **DiabetesPedigreeFunction** : diabetes percentage.
-  **Age** : age of the woman
-  **Outcome** : 1 if the woman has diabetes, 0 otherwise.

The first eight variables will be used to predict the ninth one.

The data set is available here : **[diabetes](https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset)**

\newpage

# Analysis

Once we downloaded the data set, we import it and we compute the first 5 lines :
```{r libraries importation,echo=FALSE,message=FALSE,warning=FALSE}
if(!require(readr)) install.packages('readr') else library(readr)
if(!require(tidyverse)) install.packages('tidyverse') else library(tidyverse)
if(!require(dplyr)) install.packages('dplyr') else library(dplyr)
if(!require(caret)) install.packages('caret') else library(caret)
if(!require(corrplot)) install.packages('corrplot') else library(corrplot)
if(!require(ggplot2)) install.packages('ggplot2') else library(ggplot2)
if(!require(rpart)) install.packages('rpart') else library(rpart)
if(!require(rpart.plot)) install.packages('rpart.plot') else library(rpart.plot)
if(!require(reshape2)) install.packages('reshape2') else library(reshape2)
diabetes <- data.frame(read_csv("diabetes.csv"))

```
```{r head_df, echo=FALSE,message=FALSE,warning=FALSE}
head(diabetes) %>% knitr::kable()
```

Now that we have a view on what the data set looks like, we compute some useful statistics :

- data set dimensions

```{r dimensions, echo=FALSE,message=FALSE,warning=FALSE}
data.frame(tibble("Rows"=dim(diabetes)[1],
                  "Columns"=dim(diabetes)[2])) %>% 
  knitr::kable()
```

- variables statistics 

```{r summary, echo=FALSE,message=FALSE,warning=FALSE}
summary(diabetes[1:4]) %>% knitr::kable()
summary(diabetes[5:8]) %>% knitr::kable()
```

- structure of the data set

```{r str,echo=FALSE,message=FALSE,warning=FALSE}
str(diabetes) 
```

\newpage
- proportion of Outcome values

```{r outcome_proportion, echo=FALSE,message=FALSE,warning=FALSE,fig.width=4,fig.height=4,fig.align="center"}
diabetes %>% group_by(Outcome) %>% 
  summarise(N=n()*100/dim(diabetes)[1]) %>%
  ggplot(aes(x=Outcome,y=N,fill=Outcome))+ 
  geom_text(aes(label=paste(round(N,1),"%"),vjust=-0.25,fontface='bold'))+
  geom_bar(stat = 'identity',color='black') +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.ticks.x = element_blank(),
        legend.position = "") +
  scale_x_discrete(limits=c(0,1)) +
  ggtitle("Proportions of outcomes")
```

- variables density plot

```{r variables_distribution,echo=FALSE,message=FALSE,warning=FALSE,fig.align="center",fig.height=4,fig.width=7}
ggplot(melt(diabetes),aes(x=value,fill=variable)) + 
  geom_density() + 
  facet_wrap(~variable,scale="free") + 
  theme(legend.position = "") + 
  ggtitle("Density plot for every variables")
```

\newpage
- correlation between variables

```{r correlation_plot, echo=FALSE,message=FALSE,warning=FALSE}
diabetes_abb <- diabetes
diabetes_abb_names <- c("P","G","BP","ST","I","BMI","DPF","A","OC")
colnames(diabetes_abb) <- diabetes_abb_names

cor_diabetes <- cor(diabetes_abb,method=c("spearman"))
corrplot(cor_diabetes, 
         method = 'color',
         addCoef.col = "black",
         addgrid.col = "grey",
         diag = FALSE,
         tl.pos = "d",
         tl.col = "black",
         number.cex = 0.5
         )
```

- any NaN in the set ? 

```{r nan, echo=TRUE,message=FALSE,message=FALSE,warning=FALSE}
sum(is.na(diabetes))

```

From these elements, we can extract some information :

- All variables are numerical, and the only issue we will (easily) deal with is the fact that "Outcome" is numerical instead of factor.
- Data set is clean (0 NaN).
- Approximately, a third of the patients are sick.
- It seems that there is no aberration in variables distribution.
- "Glucose,"BloodPressure" and "BMI" are close to a normal distribution (extreme values of "Glucose" tend to contradict this information).
- There is no negative correlation between variables.
- Correlation between variables and "Outcome" fluctuate from 0.07 (no correlation) to 0.48 (half correlated)

As a conclusion, we may say the data set is clean, no problem with data, and the lack of pure correlation will make this model building interesting.

\newpage

# Model building

Before we start our construction, we need to separate our set in 3 parts :

- the train set : to train our models
- the test set : to test our models performances
- the validation : to use it for the final model

We manage to do this with this piece of code :

```{r train_test_validation,echo=TRUE }
set.seed(1)
X <- diabetes %>% select(-Outcome)
y <- as.factor(diabetes$Outcome)

## Validation set
test_validation <- createDataPartition(y, times = 1, p = 0.9, list = FALSE)
validation <- diabetes[-test_validation, ]
test_index <- createDataPartition(y, times = 1, p = 0.8, list = FALSE)

## Train & Test set
train <- diabetes[test_index, ]
test <- diabetes[-test_index, ]

# Outcome as factor
train$Outcome <- as.factor(train$Outcome)
test$Outcome <- as.factor(test$Outcome)
validation$Outcome <- as.factor(validation$Outcome)
```

```{r ttv_dim, echo=FALSE}
data.frame(tibble("set"=c("train","test","validation"),"length"=c(dim(train)[1],dim(test)[1],dim(validation)[1]))) %>% knitr::kable()
```

Then, we can try our first model.

## First model

This initial model will be a "glm" one. In fact, what matters is the metrics of the model. To compute the accuracy and the f1-score on a first prediction with this code :

```{r first_fit,echo=TRUE}
# fitting
fit <- train(Outcome~.,
             data=train,
             method="glm")

# predict
pred <- predict(fit,test)

# confusion matrix
cm <- confusionMatrix(pred,test$Outcome,mode="everything",positive="1")

# accuracy
cm[3]$overall[1]
# f1-score
cm[4]$byClass[7]
```

For a first model, without any optimization, results are pretty positive. 
Now we have the methodology to fit and predict with a model, we can create a *function* to get a tibble with metrics of the models, on the train and the test set. 

```{r model_function}
model_result <- function(method){
  
  # cross-validation control
  ctrl <- trainControl(method = "repeatedcv",
                       number = 10,
                       repeats = 3)
                       
  # fitting
  fit <- train(Outcome~.,
               data=train,
               trControl = ctrl,
               method=method)

  # --- TEST ---
  # predict
  pred_test <- predict(fit,test)
  
  # confusion matrix
  cm_test <- confusionMatrix(pred_test,test$Outcome,mode="everything",positive="1")
  
  # accuracy
  accuracy_test <- round(cm_test[3]$overall[1],3)
  # f1-score
  f1_score_test <- round(cm_test[4]$byClass[7],3)
  
  # --- TRAIN ---
  # predict
  pred_train <- predict(fit,train)
  
  # confusion matrix
  cm_train <- confusionMatrix(pred_train,train$Outcome,mode="everything",positive="1")
  
  # accuracy
  accuracy_train <- round(cm_train[3]$overall[1],3)
  # f1-score
  f1_score_train <- round(cm_train[4]$byClass[7],3)
  
  
  # tibble with results
  result <- tibble("model"=method,
                   "train_acc"=accuracy_train,
                   "test_acc"=accuracy_test,
                   "acc_diff"=round(abs(accuracy_train-accuracy_test),5),
                   "train_f1"=f1_score_train,
                   "test_f1"=f1_score_test,
                   "f1_diff"=round(abs(f1_score_train-f1_score_test),5)
                   )
  return(result)
}
```

## Finding the best model

To find the best model, we will try different ones on a list. Among the most popular binary classification models, 5 were selected from the caret package :

1. svmLinear : Support Vector Machines with Linear Kernel
2. rf : Random Forest
3. glm : Generalized Linear Model (first try)
4. glmboost : Boosted Generalized Linear Model (tunable glm)
5. lda : Linear Discriminant Analysis

We proceed to study the metrics they return (this step can take few minutes to achieve):

```{r model_finding, echo=FALSE,warning=FALSE,message=FALSE, }
models <- c("svmLinear","rf","glm","glmboost","lda")
final <- data.frame()
i <- 0
for (model in models){
  #print(paste("fitting :",model))
  score <- data.frame(model_result(model))
  final <- bind_rows(final,score)
  i <- i+1
  #print(paste("[",i,"/",length(models),"] |", model, "fitted"))
}

final <- final %>% mutate(ratio = round(abs(test_acc/(acc_diff+0.2)),2))
final_table <- final[order(desc(final$ratio)),]
print(final_table)
```

To understand this results table, the "diff" columns are the differences between the train and test values of the metric. It allows use to prevent an over fitting case.
Moreover, the "ratio" columns is a homemade variable that gives us a view on which model is the best, according to the accuracy rate and the ratio.

As a conclusion, except "rf" (Random Forest), they all have results on the same range. Since "glm" and "lda" don't have values we can tune, we need to choose between "glmboost" and "svmLinear". Let's tune them both to see which one will prevail.

\newpage

## Tuning glmBoost

With the glmBoost method, two values need to be set : *mstop* and *prune*.
Prune is a "yes/no" value. By setting it to "yes", mstop will be automatically selected. But since we want to manually tune this up, we will set it as "no".

About the *mstop* value, we will set a range from 50 to 500, by steps of 10.
We create a new metric : ratio = accuracy * f1-score. This permits to find the best compromise between accuracy and f1-score by plotting ratio per mstop value. The highest the ratio, the better.

```{r mstop_finding, echo=FALSE, warning=FALSE, message=FALSE}
accuracies <- list()
f1_scores <- list()
mstops <- list()
iterations <- seq(50,500,10)

ctrl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
for (i in iterations){
  grid = expand.grid(mstop = i,
                      prune="no")
  fit <- train(Outcome~.,
                  data=train,
                  trControl=ctrl,
                  method="glmboost",
                  tuneGrid=grid)
  pred <- predict(fit,test)
  
  cm <- confusionMatrix(pred,test$Outcome,mode="everything",positive="1")
  
  mstops <- append(mstops,i)
  accuracies <- append(accuracies,accuracy_test <- cm[3]$overall[1])
  f1_scores <- append(f1_scores,accuracy_test <- cm[4]$byClass[7])
  #print(paste(round((i-min(iterations))*100/(max(iterations)-min(iterations))),"% completed"))
}
```

```{r mstop_plot,echo=FALSE}
mstops_tuning <- data.frame(tibble("mstop"=mstops,"accuracy"=accuracies,"f1_score"=f1_scores))
mstops_tuning <- as.data.frame(lapply(mstops_tuning, unlist))
mstops_tuning <- mstops_tuning %>% mutate(ratio = (accuracy*f1_score))
ggplot(mstops_tuning,aes(x=mstop,y=ratio)) + 
  geom_line() +
  ggtitle(paste("Ratio per mstop values. Max for mstop =",
                mstops_tuning$mstop[which.max(mstops_tuning$ratio)]))
```

We have the mstop value that maximize the ratio, we can now plot the result for an optimized glmboost model : 

```{r glm_opt, echo=FALSE}
grid_opt <- expand.grid(mstop = mstops_tuning$mstop[which.max(mstops_tuning$ratio)],
                       prune = 'no')

fit_opt <- train(Outcome~.,
                 data=train,
                 trControl=ctrl,
                 method="glmboost",
                 tuneGrid=grid_opt) 

pred_opt <- predict(fit_opt,test)

cm_opt <- confusionMatrix(pred_opt,
                          test$Outcome,
                          mode="everything",
                          positive="1")

glm_opt_result <- data.frame(tibble("model"    = "glmBoost",
                                    "accuracy" = round(cm_opt[3]$overall[1],3),
                                    "f1_score" = round(cm_opt[4]$byClass[7],3)))
print(glm_opt_result) %>% knitr::kable()
```

The accuracy is average, and the f1-score is not really higher the other models we tried. We will tune the svlLinear model to see if we get better results.


\newpage 

## Tuning svmLinear

The svmLiner tuning will be done the same way glmboost was. The only exception is that there is only one value to tune, instead of two : C.
A range of 0.01 to 2 with a step of 0.02 is set.

We, as previously, plot ratio (defined as accuracy*f1-score aswell) per C values.

```{r c_finding, echo=FALSE,message=FALSE,warning=FALSE}
accuracies <- list()
f1_scores <- list()
c <- list()

iterations <- seq(0.01,5,0.05)

ctrl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
for (i in iterations){
  grid = expand.grid(C = i)
  fit <- train(Outcome~.,
               data=train,
               trControl=ctrl,
               method="svmLinear",
               tuneGrid=grid)
  pred <- predict(fit,test)
  
  cm <- confusionMatrix(pred,test$Outcome,mode="everything",positive="1")
  
  c <- append(c,i)
  accuracies <- append(accuracies,accuracy_test <- cm[3]$overall[1])
  f1_scores <- append(f1_scores,accuracy_test <- cm[4]$byClass[7])
  #print(paste(round((i-min(iterations))*100/(max(iterations)-min(iterations))),"% completed"))
}
```

```{r c_plot,echo=FALSE}
c_tuning <- data.frame(tibble("C"=c,"accuracy"=accuracies,"f1_score"=f1_scores))
c_tuning <- as.data.frame(lapply(c_tuning, unlist))
c_tuning <- c_tuning %>% mutate(ratio = (accuracy*f1_score))
ggplot(c_tuning,aes(x=C,y=ratio)) + 
  geom_line() +
  ggtitle(paste("Ratio per C values. Max for C = ",
                c_tuning$C[which.max(c_tuning$ratio)]))
```

With this C value, we have these results for svmLinear :

```{r svm_opt,echo=FALSE}
grid_svm <- expand.grid(C = c_tuning$C[which.max(c_tuning$ratio)])

fit_svm <- train(Outcome~.,
                 data=train,
                 trControl=ctrl,
                 method="svmLinear",
                 tuneGrid=grid_svm) 

pred_svm <- predict(fit_svm,test)

cm_svm <- confusionMatrix(pred_svm,
                          test$Outcome,
                          mode="everything",
                          positive="1")

svm_opt_result <- data.frame(tibble("model"    = "svmLinear",
                                    "accuracy" = round(cm_svm[3]$overall[1],3),
                                    "f1_score" = round(cm_svm[4]$byClass[7],3)))
print(svm_opt_result)
```

Accuracy and F1-score are a little better than the glmboost model. We will analyze the results ine the next part.

\newpage 

# Results

Model finding results : 

```{r mf_results, echo=FALSE}
print(final_table)
```

Tuning results : 

```{r tuning_r, echo=FALSE}
final_results <- bind_rows(glm_opt_result,svm_opt_result) %>% knitr::kable()
print(final_results)
```

Among all the models we've tried, svmLinear tends to be the best. However, we can see that, unlike glmBoost that receive a growth in its metrics through tuning, svmLinear remains the same. Now we can recreate a svmLinear model from scratch to get all the information we can extract on this model performance.

```{r svm_final, echo=FALSE}
svm_fit <- train(Outcome~., data=train,trControl=ctrl,method="svmLinear")
svm_predict <- predict(svm_fit,validation)
svm_cm <- confusionMatrix(svm_predict,validation$Outcome,
                          mode="everything",
                          positive="1")

svm_final_results <- data.frame(tibble("Model" = "svmLinear",
                                       "Accuracy" = round(svm_cm[3]$overall[1],3),
                                       "F1-score" = round(svm_cm[4]$byClass[7],3),
                                       "Precision" = round(svm_cm[4]$byClass[5],3),
                                       "Recall" = round(svm_cm[4]$byClass[6],3),
                                       "Sensitivity" = round(svm_cm[4]$byClass[1],3),
                                       "Specificity" = round(svm_cm[4]$byClass[2],3)
                                       )) %>% knitr::kable()
print(svm_final_results)
```
Results on the validation set are clearly better. Let's analyze it metric per metric :

- accuracy : we saw this one earlier, it is higher than what we got first, making this model stronger.
- F1-score : it remains almost the same, so we can't conclude it gets better.
- Precision & Recall : in fact, F1-score is a sort of mean of precision and recall, that's why f1-score is worth studying.
- Sensitivity : is at a good level, but since it is defined as "true positive rate" and we face a medical case, it remains a bit low.
- specificity : this one is really high, it is defined as "true false rate" which is convenient in this case.

To understand the last two better, we can translate, for this project, the specificity as the "proportion of really sick people among all the people who were diagnosed with diabetes" and specificity as "proportion of people that are really healthy among all the ones who were diagnosed negative". Now we see why a high sensibility is mandatory in a medical study : we cannot let people get misdiagnosed. Specificity is important as well because we don't want healthy people to take a difficult treatment because they also were misdiagnosed.

Here is another metric used to clearly see how the prediction went during the use of the validation set, we call it "the confusion matrix" :

```{r svm_cm,echo=FALSE}
print(svm_cm[2]$table)
```

\newpage

# Conclusion

In conclusion, as a machine learning point of view, this final model can be useful. The metrics we saw are high enough to consider this model good. But, since we are studying a medical case, we cannot use this model in real life.

We can explain this with few points : 

- the data set is usable, but for a really precise project, we would need more data
- the people targeted by this data set is really niche since it only concerns women from a specific area
- as we saw, variables are not really correlated to the outcome, which makes it harder for our machine learning to get higher results

Finally, to have a better prediction model, we could have used Deep Learning and Neural Networks, but it is for an other level of data science and it is get outside of the machine learning field.

## Bonus : decision tree

Since data scientists need to make predicting models, but also readable reports, I chose to use an other model to have a more straightforward view on prediction : the decision tree. We fit it on train data set to get the features importance the models use. Then, we can plot a tree that explains how the model predicts class.

```{r feature_importance, echo=FALSE,warning=FALSE,message=FALSE}
tree <- rpart(Outcome~.,data=train)
importances <- data.frame(tree$variable.importance)
feature_importance <- data.frame(tibble("feature"=rownames(importances),
                                        "importance"=importances$tree.variable.importance))

feature_importance <- feature_importance[order(desc(feature_importance$importance)),]
feature_importance %>% 
  arrange(desc(importance)) %>%
  mutate(feature=factor(feature,levels=feature)) %>%
  ggplot(aes(y=importance,x=feature,fill=feature)) + 
  geom_col() + 
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "none",
        axis.text.x = element_text(angle=45
                                   ,hjust=1))+
  ggtitle("Features importance")
```
\newpage

```{r decision_tree_plot, echo=FALSE,fig.height=9,fig.width=6,fig.align="center"}
rpart.plot(tree, 
           type = 5, 
           extra = 100,
           box.palette = "GnRd",
           main="Decision tree")
```

